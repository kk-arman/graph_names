{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pathlib import Path\n",
    "from torch_geometric.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtnames_dir = Path('../../')\n",
    "src_dir = jtnames_dir / 'src'\n",
    "sys.path.append(src_dir.as_posix())\n",
    "\n",
    "from models.tgeometric.dataset import TorchGeometricDataset\n",
    "from models.tgeometric.vocabulary import Vocabulary\n",
    "from models.ggcn_lightning import GGCNLightning\n",
    "from models.tgeometric.ggcn import GGCN\n",
    "# from models.tgeometric.simple_decoder import SimpleDecoder\n",
    "\n",
    "from data.scripts_for_filtration.method_name_tokenizer import tokenize\n",
    "from utility.metrics import set_sentence_model, get_sentence_similarity\n",
    "\n",
    "gpus=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtnames = str(jtnames_dir.absolute())\n",
    "data_train = jtnames  + '/data/processed/fn_graphs_article/train'\n",
    "data_val = jtnames  + '/data/processed/fn_graphs_article/val'\n",
    "data_test = jtnames  + '/data/processed/fn_graphs_article/test'\n",
    "\n",
    "# vocabulary = Vocabulary([Path(data_train), Path(data_val), Path(data_test)], tokenize)\n",
    "# vocabulary.save(Path(vocabulary_path))\n",
    "\n",
    "vocabulary_path = jtnames + '/data/processed/fn_graphs_article/vocab.pickle'\n",
    "\n",
    "vocabulary = Vocabulary.load(Path(vocabulary_path)) # load vocab that waws used while training\n",
    "train_dataset = TorchGeometricDataset(data_train, vocabulary)\n",
    "val_dataset = TorchGeometricDataset(data_val, vocabulary)\n",
    "test_dataset = TorchGeometricDataset(data_test, vocabulary)\n",
    "\n",
    "pad_index = vocabulary.token_encoder['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # Train hparams\n",
    "    'max_epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'es_mode': 'max',\n",
    "    'es_patience': 10,\n",
    "    'es_metric': 'val_acc',\n",
    "    'lr': .005,\n",
    "    'optimizer': 'Adam',\n",
    "    'lr_scheduler': 'ReduceLROnPlateau',\n",
    "    'lr_scheduler_metric': 'val_loss',\n",
    "    # Model hparams\n",
    "    'num_agg_steps': 5,\n",
    "    'encoder_output_dim': 128,\n",
    "    'encoder_step_norm': False,\n",
    "    'node_type_embed_dim': 10,\n",
    "    'node_attr_embed_dim': 10,\n",
    "    'decoder': 'simple',\n",
    "    'decoder_hidden_size': 20,\n",
    "    'decoder_num_layers': 1,\n",
    "    # Metrics configuration\n",
    "    'sentence_encoder': 'msmarco-distilbert-base-v2',\n",
    "}\n",
    "\n",
    "model = GGCNLightning(vocabulary, pad_index, train_df = train_dataset, \n",
    "                      val_df = val_dataset, \n",
    "                      test_df = test_dataset, hparams = hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience=hparams['es_patience']\n",
    "max_epochs=hparams['max_epochs']\n",
    "\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=\"article_training_simple_old_shuffled\")\n",
    "estop = pl.callbacks.early_stopping.EarlyStopping(monitor=hparams['es_metric'], patience=patience, mode = hparams['es_mode'])\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='article_training_simple_old_shuffled/',\n",
    "    filename='article_training_simple_old_shuffled-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=gpus,\n",
    "    max_epochs=max_epochs,\n",
    "    logger=logger,\n",
    "    callbacks=[estop, checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # Train hparams\n",
    "    'max_epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'es_mode': 'max',\n",
    "    'es_patience': 10,\n",
    "    'es_metric': 'val_acc',\n",
    "    'lr': .005,\n",
    "    'optimizer': 'Adam',\n",
    "    'lr_scheduler': 'ReduceLROnPlateau',\n",
    "    'lr_scheduler_metric': 'val_acc_by_token',\n",
    "    # Model hparams\n",
    "    'num_agg_steps': 5,\n",
    "    'encoder_output_dim': 128,\n",
    "    'encoder_step_norm': False,\n",
    "    'node_type_embed_dim': 10,\n",
    "    'node_attr_embed_dim': 10,\n",
    "    'decoder': 'rnn',\n",
    "    'decoder_hidden_size': 20,\n",
    "    'decoder_num_layers': 1,\n",
    "    # Metrics configuration\n",
    "    'sentence_encoder': 'msmarco-distilbert-base-v2',\n",
    "}\n",
    "\n",
    "model = GGCNLightning(vocabulary, pad_index, train_df = train_dataset, \n",
    "                      val_df = val_dataset, \n",
    "                      test_df = test_dataset, hparams = hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience=hparams['es_patience']\n",
    "gpus=1\n",
    "max_epochs=hparams['max_epochs']\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=\"article_training_rnn_old_shuffled\")\n",
    "estop = pl.callbacks.early_stopping.EarlyStopping(monitor=hparams['es_metric'], patience=patience, mode = hparams['es_mode'])\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='article_training_rnn_old_shuffled/',\n",
    "    filename='article_training_rnn_old_shuffled-07-04-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=gpus,\n",
    "    max_epochs=max_epochs,\n",
    "    logger=logger,\n",
    "    callbacks=[estop, checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gnn",
   "language": "python",
   "name": "torch_gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}